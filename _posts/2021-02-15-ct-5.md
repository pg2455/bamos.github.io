---
layout: distillPost
title: Limitations and Open Questions
date:   2021-02-15 00:00:05
description: Contact tracing as a personalization framework
series:
 name: ct
 id: 5

distill: true
mathjax: true
bibliography: ct.bib

summary: >
  <ul>
    <li><em>Limitations to the existing method and possible research directions are discussed</em></li>
  </ul>

comments: true
---


Ideally, one would think of solving our problem as determining which individual should be in what level of user behavior restrictions.
    This sounds very much like reinforcement learning problem.
    However, given the simulations are really expensive, we couldn't afford to go this route.
    
So far, we discussed the PCT framework in the spirit of a personalization tool that incorporates relevant data points to recommend cautionary behaviors to app-users.
These recommendations are intended to control the outbreak while minimizing economic disruption.
The framework is grounded in epidemiology and virology while respecting technological constraints such as user privacy and Bluetooth communication protocol.
Finally, apart from various rule-based instantiations of the framework, we also saw a deep learning solution to enact on the framework.

Still, this is just the first step towards having a better contact tracing framework.
Hopefully, addressing some of the shortcomings of the framework can help us better prepare for the future.
While the subsequent post discusses why we should care, we focus on some of the limitations we identified and possible ways to mitigate them.

\begin{itemize}
    \item Network Optimization: Recall that the PCT framework requires sending a warning signal every time it changes for a particular day.
    This, naturally, can lead to a lot of warning signals being sent around, thereby consuming the network bandwidth more than usual.
    However, not all warning signals are going to be informative.
    For example, in a rule-based predictor, we only care about the maximum warning signal.
    Thus, a plausible solution to this problem is to have a heuristic to decide who will send a warning signal and how many of those.
    \item Reliability of the simulator: A simulator is good to the order of assumptions that went into building it.
    Even though, we used domain randomization to make the predictors robust to these assumptions, we can always expect the simulator to deviate from reality.
    A natural next step would be to incorporate real-world app usage data with the simulated data to learn a generative model infused with the epidemiological assumptions.
    Thus, in an attempt to align the simulator to reality, we think that a method like wake-sleep or variational tuning will be helpful.
    The idea will be to replace parts of the simulator with learnable functions, and use observed data to tune the weights of these functions, thereby obtaining dataset that is closer to the real data.
    Note that we don't observe ground truth in the real data.
    \item Reinforcement Learning - As mentioned earlier, the ideal formulation of the problem should be in reinforcement learning setting with actions to decide behavior levels of the users and rewards as change in prevalence of the disease.
    However, such experiments can only be carried out if we have access to faster simulators.
    \item Backward Tracing formulation - Covid-19 is an overdispersed phenomenon, in which a few people infect many.
    Thus, reproductive number for Covid-19 doesn't give a full picture of the disease.
    It is because reproductive number of $2$ implies that everyone is equally likely to infect two other susceptible individuals.
    However, due to dispersion, it can be that a few people infect 10 others on an average while some infect only 1.
    Therefore, it is increasingly important to detect the source of infection as early as possible.
    This is made possible by reserving some bits in the N-bit warning signal to indicate the likelihood of the direction of infection.
    Though, a full solution need to be patched, we think incorporating this information in the warning signal should let us identify the source.
    \item interpretable models - Deep learning models can be thought of as blackbox models.
    They are not interpretable, yet.
    Researchers are working hard to address this problem so that these models can be used seamlessly in applications of societal relevance.
    Same applies to our framework when used with the deep learning predictor.
    A user being recommended certain behavior has the right to know why is that.
    Thus, extracting why of the deep learning model is important.
    Though this hasn't been addressed in the paper, we think some more research is needed to make these deep learning predictor practical.
    \item Biases - A biased dataset leads to biased models, be it rule-based predictor or a deep learning predictor.
    If the simulator is tuned with real-world app usage data, it is likely that biases will creep in the dataset depending on where the app was launched.
    Thus, it will be necessary to understand such implications of these predictors.
    \item Offline Reinforcement Learning - A downside of using a deep learning predictor is the dependence on the simulator.
    Imagine if we were able to learn a better predictor from the current rule-based predictor.
    This way, one could launch a rule-based predictor as soon as possible and collect data to refine the rules or learn a deep learning predictor from this data.
    This is similar to the objective of offline reinforcement learning (link), where a rolled-out policy is improved via reinforcement learning offline without needing an access to the simulator.
\end{itemize}
